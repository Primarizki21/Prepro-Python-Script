{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tes Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typer\n",
    "import pandas as pd\n",
    "import re\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import pycountry\n",
    "from spellchecker import SpellChecker\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Raw Data/custdirty_data.csv\")\n",
    "df2 = pd.read_csv(\"Raw Data/government_citizenship_dirty.csv\")\n",
    "df3 = pd.read_csv(\"Raw Data/hospital_data_dirty.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Product_Category</th>\n",
       "      <th>Purchase_Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Country</th>\n",
       "      <th>Phone_Number</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CUST_001</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Mar 05 2024</td>\n",
       "      <td>65.58</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Today, federal, claim, report, consider, publi...</td>\n",
       "      <td>USA</td>\n",
       "      <td>250518 1271</td>\n",
       "      <td>eric82@example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CUST_002</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>May 29 2024</td>\n",
       "      <td>335.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Alone appear example.</td>\n",
       "      <td>Canada</td>\n",
       "      <td>891.222.8992x0833</td>\n",
       "      <td>allentodd@example.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CUST_003</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>May 28 2024</td>\n",
       "      <td>106.92</td>\n",
       "      <td>19.0</td>\n",
       "      <td>DIRECTION, SECURITY, WHO, GROUND, TRIP, ROOM. ...</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>(926)729-9917x4080</td>\n",
       "      <td>oschultz@example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CUST_004</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Jun 30 2024</td>\n",
       "      <td>389.70</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Answer, huge, six, new, tough. - picture</td>\n",
       "      <td>Canada</td>\n",
       "      <td>(421)698-4215x479</td>\n",
       "      <td>ajohnson@example.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CUST_005</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>14/10/2024</td>\n",
       "      <td>140.78</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Recent last economic establish understand.</td>\n",
       "      <td>Canada</td>\n",
       "      <td>294283 6286</td>\n",
       "      <td>andersonsheriexample.org.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10295</th>\n",
       "      <td>7431</td>\n",
       "      <td>CUST_7431</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Feb 24 2024</td>\n",
       "      <td>445.08</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Meeting, adult, choose, customer, see, base. -...</td>\n",
       "      <td>USA</td>\n",
       "      <td>785235 4238x89420</td>\n",
       "      <td>erinleon@example.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10296</th>\n",
       "      <td>940</td>\n",
       "      <td>CUST_940</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Jun 13 2024</td>\n",
       "      <td>188.90</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Actually home blue get.</td>\n",
       "      <td>Usa</td>\n",
       "      <td>380 450 1416x0693</td>\n",
       "      <td>obrienmichael@example.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10297</th>\n",
       "      <td>9519</td>\n",
       "      <td>CUST_9519</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Apr 04 2024</td>\n",
       "      <td>371.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Option stand determine course include.</td>\n",
       "      <td>Usa</td>\n",
       "      <td>434 895 9812</td>\n",
       "      <td>brownjames@example.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
       "      <td>1804</td>\n",
       "      <td>CUST_1804</td>\n",
       "      <td>Books</td>\n",
       "      <td>May 06 2024</td>\n",
       "      <td>165.60</td>\n",
       "      <td>18.0</td>\n",
       "      <td>ACCOUNT, AFFECT, NIGHT. - ATTACK</td>\n",
       "      <td>CAN</td>\n",
       "      <td>8682519899</td>\n",
       "      <td>thomas86@example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10299</th>\n",
       "      <td>8721</td>\n",
       "      <td>CUST_8721</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>2024-08-12</td>\n",
       "      <td>90.66</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Fall, task, vote, remain, my, score. - toward</td>\n",
       "      <td>Usa</td>\n",
       "      <td>(258)677-4853x19920</td>\n",
       "      <td>kyle30@example.org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10300 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID CustomerID Product_Category Purchase_Date   Price  Quantity  \\\n",
       "0         1   CUST_001      Electronics   Mar 05 2024   65.58      11.0   \n",
       "1         2   CUST_002      Electronics   May 29 2024  335.85       1.0   \n",
       "2         3   CUST_003      Electronics   May 28 2024  106.92      19.0   \n",
       "3         4   CUST_004         Clothing   Jun 30 2024  389.70       6.0   \n",
       "4         5   CUST_005        Furniture    14/10/2024  140.78      14.0   \n",
       "...     ...        ...              ...           ...     ...       ...   \n",
       "10295  7431  CUST_7431        Furniture   Feb 24 2024  445.08       6.0   \n",
       "10296   940   CUST_940         Clothing   Jun 13 2024  188.90       9.0   \n",
       "10297  9519  CUST_9519         Clothing   Apr 04 2024  371.88       NaN   \n",
       "10298  1804  CUST_1804            Books   May 06 2024  165.60      18.0   \n",
       "10299  8721  CUST_8721         Clothing    2024-08-12   90.66       4.0   \n",
       "\n",
       "                                     Product_Description         Country  \\\n",
       "0      Today, federal, claim, report, consider, publi...             USA   \n",
       "1                                  Alone appear example.          Canada   \n",
       "2      DIRECTION, SECURITY, WHO, GROUND, TRIP, ROOM. ...  United Kingdom   \n",
       "3               Answer, huge, six, new, tough. - picture          Canada   \n",
       "4             Recent last economic establish understand.          Canada   \n",
       "...                                                  ...             ...   \n",
       "10295  Meeting, adult, choose, customer, see, base. -...             USA   \n",
       "10296                            Actually home blue get.             Usa   \n",
       "10297             Option stand determine course include.             Usa   \n",
       "10298                   ACCOUNT, AFFECT, NIGHT. - ATTACK             CAN   \n",
       "10299      Fall, task, vote, remain, my, score. - toward             Usa   \n",
       "\n",
       "              Phone_Number                      Email  \n",
       "0              250518 1271         eric82@example.com  \n",
       "1        891.222.8992x0833      allentodd@example.org  \n",
       "2       (926)729-9917x4080       oschultz@example.com  \n",
       "3        (421)698-4215x479       ajohnson@example.org  \n",
       "4              294283 6286  andersonsheriexample.org.  \n",
       "...                    ...                        ...  \n",
       "10295    785235 4238x89420       erinleon@example.net  \n",
       "10296    380 450 1416x0693  obrienmichael@example.org  \n",
       "10297         434 895 9812     brownjames@example.net  \n",
       "10298           8682519899       thomas86@example.com  \n",
       "10299  (258)677-4853x19920         kyle30@example.org  \n",
       "\n",
       "[10300 rows x 10 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CitizenID</th>\n",
       "      <th>Name</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Address</th>\n",
       "      <th>VotingStatus</th>\n",
       "      <th>PassportNumber</th>\n",
       "      <th>TaxID</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>CriminalRecord</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>Income</th>\n",
       "      <th>ImmigrationYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CIT_001</td>\n",
       "      <td>Allen Novak</td>\n",
       "      <td>1968-06-16</td>\n",
       "      <td>USA</td>\n",
       "      <td>597 Carr Creek Apt. 464; West Andrew; NE 76236,</td>\n",
       "      <td>No</td>\n",
       "      <td>460200468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>HS</td>\n",
       "      <td>42533.54</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CIT_002</td>\n",
       "      <td>TAMARA RAMSEY</td>\n",
       "      <td>1987-02-11</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>3899 Medina Roads; Crystaltown; NY 04112,</td>\n",
       "      <td>No</td>\n",
       "      <td>INVALID_our</td>\n",
       "      <td>TAX-002-36</td>\n",
       "      <td>S</td>\n",
       "      <td>No</td>\n",
       "      <td>Masters</td>\n",
       "      <td>28175.78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CIT_003</td>\n",
       "      <td>David Williams</td>\n",
       "      <td>1937-12-15</td>\n",
       "      <td>Bharat</td>\n",
       "      <td>03064 Eric Ferry; Port Rebecca; AZ 09961,</td>\n",
       "      <td>Y</td>\n",
       "      <td>E48182721</td>\n",
       "      <td>TAX-003-03</td>\n",
       "      <td>Widowed</td>\n",
       "      <td></td>\n",
       "      <td>High School</td>\n",
       "      <td>143299.35</td>\n",
       "      <td>1986.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CIT_004</td>\n",
       "      <td>Desiree Willis</td>\n",
       "      <td>1991-03-24</td>\n",
       "      <td>USA</td>\n",
       "      <td>773 Perry Flats; East Michael; WY 77221,</td>\n",
       "      <td>Y</td>\n",
       "      <td>O98365870</td>\n",
       "      <td>TAX_004_11</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>105629.33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CIT_005</td>\n",
       "      <td>Terry Gill</td>\n",
       "      <td>2000-10-15</td>\n",
       "      <td>FR</td>\n",
       "      <td>214 Michael Crescent Suite 393, New Paulport, ...</td>\n",
       "      <td>N</td>\n",
       "      <td>R72171677</td>\n",
       "      <td>TAX-005-00</td>\n",
       "      <td>S</td>\n",
       "      <td>Pending</td>\n",
       "      <td>PhD</td>\n",
       "      <td>117985.07</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10495</th>\n",
       "      <td>CIT_3673</td>\n",
       "      <td>Scott Barrett</td>\n",
       "      <td>1950-03-08</td>\n",
       "      <td>IND</td>\n",
       "      <td>10041 Fisher Row Suite 135, Lake Anneburgh, GA...</td>\n",
       "      <td>N</td>\n",
       "      <td>407208444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "      <td>PhD</td>\n",
       "      <td>99415.01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10496</th>\n",
       "      <td>CIT_8693</td>\n",
       "      <td>JAMES BROWN</td>\n",
       "      <td>2000-12-13</td>\n",
       "      <td>US</td>\n",
       "      <td>563 Romero Ferry, Petersonport, OR 06511</td>\n",
       "      <td>N</td>\n",
       "      <td>983829316</td>\n",
       "      <td>TAX-8693-20</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhD</td>\n",
       "      <td>128327.81</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10497</th>\n",
       "      <td>CIT_7140</td>\n",
       "      <td>Susan Beck</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>US</td>\n",
       "      <td>31275 Michelle Well Suite 725, West Randytown,...</td>\n",
       "      <td></td>\n",
       "      <td>INVALID_enjoy</td>\n",
       "      <td>TAX-7140-50</td>\n",
       "      <td>Single</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>26909.17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>CIT_779</td>\n",
       "      <td>JACKSON STOUT</td>\n",
       "      <td>1983-08-18</td>\n",
       "      <td>India</td>\n",
       "      <td>10300 Michelle Passage, Amandaburgh, RI 93986</td>\n",
       "      <td>Registered</td>\n",
       "      <td>O91186652</td>\n",
       "      <td>TAX-779-15</td>\n",
       "      <td>S</td>\n",
       "      <td>No</td>\n",
       "      <td>High School</td>\n",
       "      <td>132278.13</td>\n",
       "      <td>1969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>CIT_2923</td>\n",
       "      <td>DAWN LANE</td>\n",
       "      <td>2002-12-23</td>\n",
       "      <td>United States</td>\n",
       "      <td>07158 Tamara Drives Apt. 970, Brooksfurt, CT 3...</td>\n",
       "      <td>No</td>\n",
       "      <td>S29245729</td>\n",
       "      <td>TAX-2923-67</td>\n",
       "      <td>Married</td>\n",
       "      <td>Yes</td>\n",
       "      <td>PhD</td>\n",
       "      <td>52938.39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10500 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CitizenID            Name   BirthDate    Nationality  \\\n",
       "0       CIT_001     Allen Novak  1968-06-16            USA   \n",
       "1       CIT_002   TAMARA RAMSEY  1987-02-11         FRANCE   \n",
       "2       CIT_003  David Williams  1937-12-15         Bharat   \n",
       "3       CIT_004  Desiree Willis  1991-03-24            USA   \n",
       "4       CIT_005      Terry Gill  2000-10-15             FR   \n",
       "...         ...             ...         ...            ...   \n",
       "10495  CIT_3673   Scott Barrett  1950-03-08            IND   \n",
       "10496  CIT_8693     JAMES BROWN  2000-12-13             US   \n",
       "10497  CIT_7140      Susan Beck  2001-05-11             US   \n",
       "10498   CIT_779   JACKSON STOUT  1983-08-18          India   \n",
       "10499  CIT_2923       DAWN LANE  2002-12-23  United States   \n",
       "\n",
       "                                                 Address VotingStatus  \\\n",
       "0        597 Carr Creek Apt. 464; West Andrew; NE 76236,           No   \n",
       "1              3899 Medina Roads; Crystaltown; NY 04112,           No   \n",
       "2              03064 Eric Ferry; Port Rebecca; AZ 09961,            Y   \n",
       "3               773 Perry Flats; East Michael; WY 77221,            Y   \n",
       "4      214 Michael Crescent Suite 393, New Paulport, ...            N   \n",
       "...                                                  ...          ...   \n",
       "10495  10041 Fisher Row Suite 135, Lake Anneburgh, GA...            N   \n",
       "10496           563 Romero Ferry, Petersonport, OR 06511            N   \n",
       "10497  31275 Michelle Well Suite 725, West Randytown,...                \n",
       "10498      10300 Michelle Passage, Amandaburgh, RI 93986   Registered   \n",
       "10499  07158 Tamara Drives Apt. 970, Brooksfurt, CT 3...           No   \n",
       "\n",
       "      PassportNumber        TaxID MaritalStatus CriminalRecord EducationLevel  \\\n",
       "0          460200468          NaN       Married             No             HS   \n",
       "1        INVALID_our   TAX-002-36             S             No        Masters   \n",
       "2          E48182721   TAX-003-03       Widowed                   High School   \n",
       "3          O98365870   TAX_004_11       Married             No      Doctorate   \n",
       "4          R72171677   TAX-005-00             S        Pending            PhD   \n",
       "...              ...          ...           ...            ...            ...   \n",
       "10495      407208444          NaN       Unknown                           PhD   \n",
       "10496      983829316  TAX-8693-20       Unknown            NaN            PhD   \n",
       "10497  INVALID_enjoy  TAX-7140-50        Single            Yes    High School   \n",
       "10498      O91186652   TAX-779-15             S             No    High School   \n",
       "10499      S29245729  TAX-2923-67       Married            Yes            PhD   \n",
       "\n",
       "          Income  ImmigrationYear  \n",
       "0       42533.54              NaN  \n",
       "1       28175.78              NaN  \n",
       "2      143299.35           1986.0  \n",
       "3      105629.33              NaN  \n",
       "4      117985.07           2005.0  \n",
       "...          ...              ...  \n",
       "10495   99415.01              NaN  \n",
       "10496  128327.81              NaN  \n",
       "10497   26909.17              NaN  \n",
       "10498  132278.13           1969.0  \n",
       "10499   52938.39              NaN  \n",
       "\n",
       "[10500 rows x 13 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>AdmissionDate</th>\n",
       "      <th>DischargeDate</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Doctor</th>\n",
       "      <th>Department</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BloodType</th>\n",
       "      <th>Medication</th>\n",
       "      <th>TestResults</th>\n",
       "      <th>BillingAmount</th>\n",
       "      <th>InsuranceProvider</th>\n",
       "      <th>AppointmentDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PT_001</td>\n",
       "      <td>2024-10-03</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>Appendicitis</td>\n",
       "      <td>Special with job,</td>\n",
       "      <td>Carl Beard</td>\n",
       "      <td>Orthopedics</td>\n",
       "      <td>81</td>\n",
       "      <td>Female</td>\n",
       "      <td>B+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85/67</td>\n",
       "      <td>9495.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PT_K_2</td>\n",
       "      <td>2023-06-27</td>\n",
       "      <td>2023-12-05</td>\n",
       "      <td>Fracture</td>\n",
       "      <td>Certainly way ten,</td>\n",
       "      <td>Mary Sanchez</td>\n",
       "      <td>Endo</td>\n",
       "      <td>-88</td>\n",
       "      <td>M</td>\n",
       "      <td>O-</td>\n",
       "      <td>civil</td>\n",
       "      <td>140/118</td>\n",
       "      <td>8681.08</td>\n",
       "      <td>HC Insurance</td>\n",
       "      <td>2023-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PT_003</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>2025-04-29</td>\n",
       "      <td>Appendicitis</td>\n",
       "      <td>Study small,</td>\n",
       "      <td>Mr. Patrick Harris MD</td>\n",
       "      <td>Orthopedics</td>\n",
       "      <td>94</td>\n",
       "      <td>M</td>\n",
       "      <td>O-</td>\n",
       "      <td>word</td>\n",
       "      <td>92/95</td>\n",
       "      <td>2829.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PT_004</td>\n",
       "      <td>Mar 11 1978</td>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>COPD</td>\n",
       "      <td>Investment human,</td>\n",
       "      <td>Donald Roach</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>26</td>\n",
       "      <td>F</td>\n",
       "      <td>A+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140/119</td>\n",
       "      <td>1121.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PT_005</td>\n",
       "      <td>2024-11-23</td>\n",
       "      <td>2024-12-02</td>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Every coach,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ortho</td>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>B-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71/68</td>\n",
       "      <td>5555.75</td>\n",
       "      <td>HC Insurance</td>\n",
       "      <td>2023-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10295</th>\n",
       "      <td>PT_691</td>\n",
       "      <td>2024-03-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COPD</td>\n",
       "      <td>Focus population green,</td>\n",
       "      <td>Jennifer Rodriguez</td>\n",
       "      <td>Cardio</td>\n",
       "      <td>68</td>\n",
       "      <td>Female</td>\n",
       "      <td>O+</td>\n",
       "      <td>step</td>\n",
       "      <td>High</td>\n",
       "      <td>5014.95</td>\n",
       "      <td>HealthPlus</td>\n",
       "      <td>2024-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10296</th>\n",
       "      <td>PT_7247</td>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>Fracture</td>\n",
       "      <td>Hold,</td>\n",
       "      <td>Christine Morris</td>\n",
       "      <td>Ortho</td>\n",
       "      <td>15</td>\n",
       "      <td>F</td>\n",
       "      <td>O-</td>\n",
       "      <td>floor 1 units</td>\n",
       "      <td>98/68</td>\n",
       "      <td>8152.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10297</th>\n",
       "      <td>PT_4814</td>\n",
       "      <td>Oct 11 1998</td>\n",
       "      <td>2005-04-22</td>\n",
       "      <td>Fracture</td>\n",
       "      <td>Imagine power PM,</td>\n",
       "      <td>Luis May</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>A+</td>\n",
       "      <td>senior</td>\n",
       "      <td>181/80</td>\n",
       "      <td>2323.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
       "      <td>PT_8633</td>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>2024-11-28</td>\n",
       "      <td>Dyabetes</td>\n",
       "      <td>Morning body than,</td>\n",
       "      <td>Jamie King</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>52</td>\n",
       "      <td>M</td>\n",
       "      <td>AB+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169/46</td>\n",
       "      <td>3466.39</td>\n",
       "      <td>HealthPlus</td>\n",
       "      <td>2024-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10299</th>\n",
       "      <td>PT_2977</td>\n",
       "      <td>Mar 31 2017</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>COPD</td>\n",
       "      <td>Notice after staff,</td>\n",
       "      <td>Thomas Perry</td>\n",
       "      <td>Cardilogy</td>\n",
       "      <td>89</td>\n",
       "      <td>U</td>\n",
       "      <td>A+</td>\n",
       "      <td>edge</td>\n",
       "      <td>72/46</td>\n",
       "      <td>5592.93</td>\n",
       "      <td>MediCare</td>\n",
       "      <td>2024-01-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10300 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PatientID AdmissionDate DischargeDate     Diagnosis  \\\n",
       "0        PT_001    2024-10-03    2025-05-11  Appendicitis   \n",
       "1        PT_K_2    2023-06-27    2023-12-05      Fracture   \n",
       "2        PT_003    2024-05-18    2025-04-29  Appendicitis   \n",
       "3        PT_004   Mar 11 1978    2019-10-26          COPD   \n",
       "4        PT_005    2024-11-23    2024-12-02      Diabetes   \n",
       "...         ...           ...           ...           ...   \n",
       "10295    PT_691    2024-03-16           NaN          COPD   \n",
       "10296   PT_7247    2023-03-29    2024-10-27      Fracture   \n",
       "10297   PT_4814   Oct 11 1998    2005-04-22      Fracture   \n",
       "10298   PT_8633    2024-02-01    2024-11-28      Dyabetes   \n",
       "10299   PT_2977   Mar 31 2017    2018-02-01          COPD   \n",
       "\n",
       "                     Treatment                 Doctor     Department  Age  \\\n",
       "0            Special with job,             Carl Beard    Orthopedics   81   \n",
       "1           Certainly way ten,           Mary Sanchez           Endo  -88   \n",
       "2                 Study small,  Mr. Patrick Harris MD    Orthopedics   94   \n",
       "3            Investment human,           Donald Roach  Endocrinology   26   \n",
       "4                 Every coach,                    NaN          Ortho   35   \n",
       "...                        ...                    ...            ...  ...   \n",
       "10295  Focus population green,     Jennifer Rodriguez         Cardio   68   \n",
       "10296                    Hold,       Christine Morris          Ortho   15   \n",
       "10297        Imagine power PM,               Luis May  Endocrinology    3   \n",
       "10298       Morning body than,             Jamie King     Cardiology   52   \n",
       "10299      Notice after staff,           Thomas Perry      Cardilogy   89   \n",
       "\n",
       "       Gender BloodType     Medication TestResults  BillingAmount  \\\n",
       "0      Female        B+            NaN       85/67        9495.73   \n",
       "1           M        O-          civil     140/118        8681.08   \n",
       "2           M        O-           word       92/95        2829.42   \n",
       "3           F        A+            NaN     140/119        1121.27   \n",
       "4      Female        B-            NaN       71/68        5555.75   \n",
       "...       ...       ...            ...         ...            ...   \n",
       "10295  Female        O+           step        High        5014.95   \n",
       "10296       F        O-  floor 1 units       98/68        8152.91   \n",
       "10297    Male        A+         senior      181/80        2323.13   \n",
       "10298       M       AB+            NaN      169/46        3466.39   \n",
       "10299       U        A+           edge       72/46        5592.93   \n",
       "\n",
       "      InsuranceProvider AppointmentDate  \n",
       "0                   NaN      2024-02-17  \n",
       "1          HC Insurance      2023-08-08  \n",
       "2                   NaN      2024-05-21  \n",
       "3                   NaN      2024-11-03  \n",
       "4          HC Insurance      2023-11-14  \n",
       "...                 ...             ...  \n",
       "10295        HealthPlus      2024-04-21  \n",
       "10296               NaN      2023-12-14  \n",
       "10297               NaN      2024-06-13  \n",
       "10298        HealthPlus      2024-06-26  \n",
       "10299          MediCare      2024-01-24  \n",
       "\n",
       "[10300 rows x 15 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Appendicitis', 'Fracture', 'COPD', 'Diabetes', 'Hypertension',\n",
       "       'copd', 'appendicitis', 'Hypertensyon', 'Dyabetes', 'Appendycytys',\n",
       "       'hypertension', 'fracture', 'diabetes'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['Diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                        0\n",
      "CustomerID              514\n",
      "Product_Category       1309\n",
      "Purchase_Date             0\n",
      "Price                     0\n",
      "Quantity                765\n",
      "Product_Description       0\n",
      "Country                   0\n",
      "Phone_Number              0\n",
      "Email                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df1.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CitizenID             0\n",
      "Name                  0\n",
      "BirthDate             0\n",
      "Nationality           0\n",
      "Address               0\n",
      "VotingStatus          0\n",
      "PassportNumber        0\n",
      "TaxID              1082\n",
      "MaritalStatus         0\n",
      "CriminalRecord     2093\n",
      "EducationLevel        0\n",
      "Income                0\n",
      "ImmigrationYear    7893\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df2.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatientID               0\n",
      "AdmissionDate           0\n",
      "DischargeDate        3052\n",
      "Diagnosis               0\n",
      "Treatment               0\n",
      "Doctor               1059\n",
      "Department              0\n",
      "Age                     0\n",
      "Gender                  0\n",
      "BloodType               0\n",
      "Medication           4151\n",
      "TestResults             0\n",
      "BillingAmount           0\n",
      "InsuranceProvider    3685\n",
      "AppointmentDate         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df3.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolom ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10300"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF 1 ['ID', 'CustomerID']\n",
      "DF 2 ['CitizenID', 'TaxID']\n",
      "DF 3 ['PatientID']\n"
     ]
    }
   ],
   "source": [
    "def apakah_kolom_id(kolom):\n",
    "    pattern = r'(id)[_A-Z]?'\n",
    "    return re.search(pattern, kolom, re.IGNORECASE)\n",
    "\n",
    "def identifikasi_kolom_id(df, threshold = 0.7):\n",
    "    kolom_id = []\n",
    "    for kolom in df.columns:\n",
    "        ratio = df[kolom].nunique() / len(df)\n",
    "        if (ratio >= threshold) and apakah_kolom_id(kolom):\n",
    "            kolom_id.append(kolom)\n",
    "    return kolom_id\n",
    "\n",
    "print(f\"DF 1 {identifikasi_kolom_id(df1)}\")\n",
    "print(f\"DF 2 {identifikasi_kolom_id(df2)}\")\n",
    "print(f\"DF 3 {identifikasi_kolom_id(df3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product_Category       1309\n",
      "Purchase_Date             0\n",
      "Price                     0\n",
      "Quantity                765\n",
      "Product_Description       0\n",
      "Country                   0\n",
      "Phone_Number              0\n",
      "Email                     0\n",
      "dtype: int64\n",
      "Name                  0\n",
      "BirthDate             0\n",
      "Nationality           0\n",
      "Address               0\n",
      "VotingStatus          0\n",
      "PassportNumber        0\n",
      "MaritalStatus         0\n",
      "CriminalRecord     2093\n",
      "EducationLevel        0\n",
      "Income                0\n",
      "ImmigrationYear    7893\n",
      "dtype: int64\n",
      "AdmissionDate           0\n",
      "DischargeDate        3052\n",
      "Diagnosis               0\n",
      "Treatment               0\n",
      "Doctor               1059\n",
      "Department              0\n",
      "Age                     0\n",
      "Gender                  0\n",
      "BloodType               0\n",
      "Medication           4151\n",
      "TestResults             0\n",
      "BillingAmount           0\n",
      "InsuranceProvider    3685\n",
      "AppointmentDate         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "kolom_id1 = identifikasi_kolom_id(df1)\n",
    "kolom_id2 = identifikasi_kolom_id(df2)\n",
    "kolom_id3 = identifikasi_kolom_id(df3)\n",
    "\n",
    "# df11 = df1.copy()\n",
    "# df22 = df2.copy()\n",
    "# df33 = df3.copy()\n",
    "\n",
    "df1 = df1.drop(columns=kolom_id1)\n",
    "df2 = df2.drop(columns=kolom_id2)\n",
    "df3 = df3.drop(columns=kolom_id3)\n",
    "\n",
    "print(df1.isna().sum())\n",
    "print(df2.isna().sum())\n",
    "print(df3.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Nama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           Allen Novak\n",
       "1         Tamara Ramsey\n",
       "2        David Williams\n",
       "3        Desiree Willis\n",
       "4            Terry Gill\n",
       "              ...      \n",
       "10495     Scott Barrett\n",
       "10496       James Brown\n",
       "10497        Susan Beck\n",
       "10498     Jackson Stout\n",
       "10499         Dawn Lane\n",
       "Name: Name, Length: 10500, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apakah_kolom_nama(kolom):\n",
    "    pattern = r'(name|nama|pengguna)[_A-Z]?'\n",
    "    return re.search(pattern, kolom, re.IGNORECASE)\n",
    "\n",
    "def identifikasi_kolom_nama(df, threshold = 0.7):\n",
    "    kolom_id = []\n",
    "    for kolom in df.columns:\n",
    "        ratio = df[kolom].nunique() / len(df)\n",
    "        if (ratio >= threshold) and apakah_kolom_nama(kolom):\n",
    "            kolom_id.append(kolom)\n",
    "    return kolom_id\n",
    "    \n",
    "kolom_nama = identifikasi_kolom_nama(df2)\n",
    "\n",
    "def format_name(name):\n",
    "    return name.title()\n",
    "\n",
    "for kolom in kolom_nama:\n",
    "    df2[kolom] = df2[kolom].apply(format_name)\n",
    "\n",
    "df2['Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Tanggal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Mar 05 2024\n",
       "1        May 29 2024\n",
       "2        May 28 2024\n",
       "3        Jun 30 2024\n",
       "4         14/10/2024\n",
       "            ...     \n",
       "10295    Feb 24 2024\n",
       "10296    Jun 13 2024\n",
       "10297    Apr 04 2024\n",
       "10298    May 06 2024\n",
       "10299     2024-08-12\n",
       "Name: Purchase_Date, Length: 10300, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Purchase_Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1968-06-16\n",
       "1        1987-02-11\n",
       "2        1937-12-15\n",
       "3        1991-03-24\n",
       "4        2000-10-15\n",
       "            ...    \n",
       "10495    1950-03-08\n",
       "10496    2000-12-13\n",
       "10497    2001-05-11\n",
       "10498    1983-08-18\n",
       "10499    2002-12-23\n",
       "Name: BirthDate, Length: 10500, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"BirthDate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df2[\"BirthDate\"].dtype == \"datetime64[ns]\":\n",
    "    print(\"mantap\")\n",
    "# df1['updateorder'] = np.random.randint(1, 101, size=len(df1))\n",
    "# df1['purchasedate'] = np.random.randint(1, 101, size=len(df1))\n",
    "# df1['updateorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006796116504854369\n",
      "None\n",
      "0.10660194174757281\n",
      "<re.Match object; span=(8, 13), match='_Date'>\n",
      "0.8632038834951457\n",
      "None\n",
      "0.0018446601941747574\n",
      "None\n",
      "0.970873786407767\n",
      "None\n",
      "0.0006796116504854369\n",
      "None\n",
      "0.970873786407767\n",
      "None\n",
      "0.9544660194174758\n",
      "None\n",
      "['Purchase_Date']\n",
      "['BirthDate']\n",
      "['AdmissionDate', 'DischargeDate', 'AppointmentDate']\n"
     ]
    }
   ],
   "source": [
    "def apakah_kolom_date(kolom):\n",
    "    \"\"\"\n",
    "    Apakah Kolom Tanggal?\n",
    "    Pola kata dari kolom yang ingin dihapus\n",
    "    Mencari kolom dengan pola kata \"date\" dll\n",
    "    \"\"\"\n",
    "    pattern1 = r'(date|tanggal|tgl|dt)\\b|_(date|tanggal|tgl|dt)[_A-Z]?'\n",
    "    verif1 = re.search(pattern1, kolom, re.IGNORECASE)\n",
    "    # verif2 = re.search(pattern2, kolom, re.IGNORECASE)\n",
    "    return verif1\n",
    "    # return re.search(pattern, kolom, re.IGNORECASE)\n",
    "\n",
    "def identifikasi_kolom_date(df):\n",
    "    \"\"\"\n",
    "    Identifikasi Kolom ID\n",
    "    \"\"\"\n",
    "    kolom_id = []\n",
    "    for kolom in df.columns:\n",
    "        if apakah_kolom_date(kolom):\n",
    "            kolom_id.append(kolom)\n",
    "    return kolom_id\n",
    "\n",
    "for x in df1.columns:\n",
    "    ratio = df1[x].nunique() / len(df1)\n",
    "    print(ratio)\n",
    "    print(apakah_kolom_date(x))\n",
    "print(identifikasi_kolom_date(df1))\n",
    "print(identifikasi_kolom_date(df2))\n",
    "print(identifikasi_kolom_date(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Purchase_Date']\n",
      "['BirthDate']\n",
      "['AdmissionDate', 'DischargeDate', 'AppointmentDate']\n"
     ]
    }
   ],
   "source": [
    "formats = [\n",
    "    '%d/%m/%y',  # dd/mm/yy\n",
    "    '%d/%m/%Y',  # dd/mm/yyyy\n",
    "    '%Y-%m-%d',  # yyyy-mm-dd\n",
    "    '%b %d %Y',  # Mmm dd yyyy\n",
    "    '%d %B %Y'   # dd Mmmm yyyy\n",
    "]\n",
    "\n",
    "def try_parsing_date(date_str, formats):\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return pd.NaT\n",
    "    \n",
    "def safe_parse(date_str):\n",
    "    date_str = str(date_str).strip()\n",
    "    try:\n",
    "        return parser.parse(date_str)\n",
    "    except (ValueError, TypeError): \n",
    "        return pd.NaT \n",
    "# kolom_date = identifikasi_kolom_date(df1)\n",
    "# for kolom in kolom_date:\n",
    "#     df1[kolom] = df1[kolom].apply(parser.parse)\n",
    "kolom_date1 = identifikasi_kolom_date(df1)\n",
    "kolom_date2 = identifikasi_kolom_date(df2)\n",
    "kolom_date3 = identifikasi_kolom_date(df3)\n",
    "\n",
    "print(kolom_date1)\n",
    "print(kolom_date2)\n",
    "print(kolom_date3)\n",
    "\n",
    "# df2['BirthDate'] = df2['BirthDate'].apply(safe_parse)\n",
    "df2['BirthDate'] = df2['BirthDate'].apply(safe_parse)\n",
    "# df2['BirthDate']\n",
    "\n",
    "# df1[\"Purchase_Date\"] = df1['Purchase_Date'].apply(parser.parse)\n",
    "# df1['Purchase_Date']\n",
    "# df1['Purchase_Date_Baru'] = df1['Purchase_Date'].apply(lambda x: try_parsing_date(x, formats))\n",
    "# df1['Purchase_Date_Baru']\n",
    "# df1['Purchase_Date_Baru'] = df1['Purchase_Date'].apply(parser.parse)\n",
    "# df1['Purchase_Date_Baru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulan_dict = {\n",
    "#     'januari': '01', 'februari': '02', 'maret': '03', 'april': '04',\n",
    "#     'mei': '05', 'juni': '06', 'juli': '07', 'agustus': '08',\n",
    "#     'september': '09', 'oktober': '10', 'november': '11', 'desember': '12'\n",
    "# }\n",
    "# def clean_date(date_str):\n",
    "#     date_str = str(date_str).lower()\n",
    "#     for bulan, angka in bulan_dict.items():\n",
    "#         date_str = date_str.replace(bulan, angka)  # Ganti nama bulan dengan angka\n",
    "#     return date_str\n",
    "\n",
    "# df11['DateClean'] = df11['Purchase_Date'].apply(clean_date)\n",
    "# df11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kolom country\n",
      "DF 1 ['Country']\n",
      "DF 2 ['Nationality']\n",
      "DF 3 []\n"
     ]
    }
   ],
   "source": [
    "def apakah_kolom_country(kolom):\n",
    "    pattern = r'(origin|citizenship|country|nationality)\\b|_(origin|citizenship|country|nationality)[_A-Z]?'\n",
    "    return re.search(pattern, kolom, re.IGNORECASE)\n",
    "\n",
    "def identifikasi_kolom_country(df):\n",
    "    kolom_id = []\n",
    "    for kolom in df.columns:\n",
    "        ratio = df[kolom].nunique() / len(df)\n",
    "        if apakah_kolom_country(kolom):\n",
    "            kolom_id.append(kolom)\n",
    "    return kolom_id\n",
    "\n",
    "def apakah_kolom_telp(kolom):\n",
    "    pattern = r'\\b(phone|telp|telephone)|(phone|telp|telephone)\\b|_(phone|telp|telephone)[_A-Z]?'\n",
    "    return re.search(pattern, kolom, re.IGNORECASE)\n",
    "\n",
    "def identifikasi_kolom_telp(df, threshold = 0.7):\n",
    "    kolom_id = []\n",
    "    for kolom in df.columns:\n",
    "        ratio = df[kolom].nunique() / len(df)\n",
    "        if (ratio >= threshold) and apakah_kolom_telp(kolom):\n",
    "            kolom_id.append(kolom)\n",
    "    return kolom_id\n",
    "\n",
    "print(\"kolom country\")\n",
    "print(f\"DF 1 {identifikasi_kolom_country(df1)}\")\n",
    "print(f\"DF 2 {identifikasi_kolom_country(df2)}\")\n",
    "print(f\"DF 3 {identifikasi_kolom_country(df3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USA' 'Canada' 'United Kingdom' 'UK' 'CAN' 'usa' 'Usa']\n",
      "['USA' 'FRANCE' 'Bharat' 'FR' 'America' 'France' 'India' 'IND' 'US'\n",
      " 'United States']\n"
     ]
    }
   ],
   "source": [
    "print(df1['Country'].unique())\n",
    "print(df2['Nationality'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['USA', 'Canada', 'United Kingdom', 'UK', 'CAN', 'usa', 'Usa'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA\n"
     ]
    }
   ],
   "source": [
    "country = \"usa\"\n",
    "country = country.upper()\n",
    "print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['United States' 'Canada' 'United Kingdom']\n",
      "['United States' 'France' 'India']\n"
     ]
    }
   ],
   "source": [
    "special_mapping = {\n",
    "    'america': 'USA',\n",
    "    'bharat': 'IND',\n",
    "    'uk' : 'United Kingdom'\n",
    "}\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def format_country(country):\n",
    "    country = country.lower()\n",
    "    if country in special_mapping:\n",
    "        country = special_mapping[country]\n",
    "    hasil_cari = pycountry.countries.search_fuzzy(country)[0]\n",
    "    return hasil_cari.name\n",
    "\n",
    "# def format_country(country):\n",
    "#     country = country.upper()\n",
    "#     hasil_cari = pycountry.countries.search_fuzzy(country)\n",
    "#     if hasil_cari:\n",
    "#         # Pastikan hasil yang dipilih adalah yang paling tepat, misalnya berdasarkan panjang nama\n",
    "#         return max(hasil_cari, key=lambda x: len(x.name)).name\n",
    "#     return country\n",
    "\n",
    "df1['Country'] = df1['Country'].apply(format_country)\n",
    "df2['Nationality'] = df2['Nationality'].apply(format_country)\n",
    "print(df1['Country'].unique())\n",
    "print(df2['Nationality'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Nomor Telepon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kolom telp\n",
      "DF 1 ['Phone_Number']\n",
      "DF 2 []\n",
      "DF 3 []\n"
     ]
    }
   ],
   "source": [
    "print(\"kolom telp\")\n",
    "print(f\"DF 1 {identifikasi_kolom_telp(df1)}\")\n",
    "print(f\"DF 2 {identifikasi_kolom_telp(df2)}\")\n",
    "print(f\"DF 3 {identifikasi_kolom_telp(df3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angka = [str(i) for i in range(0,10)]\n",
    "def ekstrak_simbol(baris):\n",
    "    simbol = set()\n",
    "    for nomor in baris:\n",
    "        for x in nomor:\n",
    "            if x not in angka and x not in simbol:\n",
    "                simbol.add(x)\n",
    "    return re.escape(''.join(simbol))\n",
    "\n",
    "def format_nomor(nomor, simbol, pemisah=\"-\"):\n",
    "    clean_text = re.sub(f'[{simbol}]', '', nomor)\n",
    "    chunk = []\n",
    "    for i in range(0, len(clean_text), 3):\n",
    "        chunk.append(clean_text[i:i+3])\n",
    "    hasil = pemisah.join(chunk)\n",
    "    return hasil\n",
    "\n",
    "    \n",
    "def panjang_nomor(nomor):\n",
    "    panjang = set()\n",
    "    for x in nomor:\n",
    "        panjang.add(len(x))\n",
    "    return list(panjang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencari Kolom Nomor Telepon\n",
    "def apakah_kolom_telp(kolom):\n",
    "    pattern = r'\\b(phone|telp|telephone)|(phone|telp|telephone)\\b|_(phone|telp|telephone)[_A-Z]?'\n",
    "    return re.search(pattern, kolom, re.IGNORECASE)\n",
    "\n",
    "def identifikasi_kolom_telp(df, threshold = 0.7):\n",
    "    \"\"\"\n",
    "    Identifikasi Kolom Nomor Telepon\n",
    "    \"\"\"\n",
    "    kolom_id = []\n",
    "    for kolom in df.columns:\n",
    "        ratio = df[kolom].nunique() / len(df)\n",
    "        if (ratio >= threshold) and apakah_kolom_telp(kolom):\n",
    "            kolom_id.append(kolom)\n",
    "    return kolom_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Nomor Telepon\n",
    "angka = [str(i) for i in range(0,10)]\n",
    "def ekstrak_simbol(baris):\n",
    "    \"\"\"\n",
    "    Ekstrak Simbol agar bisa digunakan disemua situasi\n",
    "    \"\"\"\n",
    "    simbol = set()\n",
    "    for nomor in baris:\n",
    "        for x in nomor:\n",
    "            if x not in angka and x not in simbol:\n",
    "                simbol.add(x)\n",
    "    return re.escape(''.join(simbol))\n",
    "\n",
    "def format_nomor(nomor, simbol, pemisah=\"-\"):\n",
    "    \"\"\"\n",
    "    Format Nomor untuk menghilangkan simbol yang tidak perlu\n",
    "    \"\"\"\n",
    "    clean_text = re.sub(f'[{simbol}]', '', nomor)\n",
    "    chunk = []\n",
    "    for i in range(0, len(clean_text), 3):\n",
    "        chunk.append(clean_text[i:i+3])\n",
    "    hasil = pemisah.join(chunk)\n",
    "    return hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ \\)\\-\\(x\\+\\.\n",
      "[\\ \\)\\-\\(x\\+\\.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0              250-518-127-1\n",
       "1         891-222-899-208-33\n",
       "2         926-729-991-740-80\n",
       "3          421-698-421-547-9\n",
       "4              294-283-628-6\n",
       "                ...         \n",
       "10295    785-235-423-889-420\n",
       "10296     380-450-141-606-93\n",
       "10297          434-895-981-2\n",
       "10298          868-251-989-9\n",
       "10299    258-677-485-319-920\n",
       "Name: Phone_Number, Length: 10300, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasil = ekstrak_simbol(df1['Phone_Number'])\n",
    "print(hasil)\n",
    "print(f'[{hasil}]')\n",
    "# hasil_panjang = panjang_nomor(df1['Phone_Number'])\n",
    "# print(hasil_panjang)\n",
    "df1['Phone_Number'] = df1['Phone_Number'].apply(lambda x:\n",
    "format_nomor(x, hasil))\n",
    "df1['Phone_Number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF 1 ['Product_Category', 'Product_Description']\n",
      "DF 2 []\n",
      "DF 3 []\n"
     ]
    }
   ],
   "source": [
    "def apakah_kolom_product(kolom):\n",
    "    pattern = r'\\b(product|category|produk|kategori)|(product|category|produk|kategori)\\b|_(product|category|produk|kategori)[_A-Z]?'\n",
    "    return re.search(pattern, kolom, re.IGNORECASE)\n",
    "\n",
    "def identifikasi_kolom_product(df):\n",
    "    kolom_id = []\n",
    "    for kolom in df.columns:\n",
    "        ratio = df[kolom].nunique() / len(df)\n",
    "        if apakah_kolom_product(kolom):\n",
    "            kolom_id.append(kolom)\n",
    "    return kolom_id\n",
    "\n",
    "print(f\"DF 1 {identifikasi_kolom_product(df1)}\")\n",
    "print(f\"DF 2 {identifikasi_kolom_product(df2)}\")\n",
    "print(f\"DF 3 {identifikasi_kolom_product(df3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1309)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"Product_Category\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Electronics', 'Clothing', 'Furniture', 'Sports', 'Grocery',\n",
       "       'Electornics', nan, 'Books'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"Product_Category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typo_mapping = {\n",
    "#     'electornics': 'electronics',\n",
    "# }\n",
    "\n",
    "# def frequncy_dict(df, kolom):\n",
    "#     return df[kolom].value_counts().to_dict()\n",
    "\n",
    "# def format_product(produk):\n",
    "#     produk = str(produk).lower()\n",
    "#     if produk in ['na','nan']:\n",
    "#         produk = 'Unknown'\n",
    "#     if produk in typo_mapping:\n",
    "#         produk = typo_mapping[produk]\n",
    "#     return produk.title()\n",
    "\n",
    "# df1['Product_Category'] = df1['Product_Category'].apply(format_product)\n",
    "# df1['Product_Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF 1 []\n",
      "DF 2 []\n",
      "DF 3 ['Age']\n"
     ]
    }
   ],
   "source": [
    "def apakah_kolom_age(kolom):\n",
    "    pattern = r'\\b(umur|age|usia)|(umur|age|usia)\\b|_(umur|age|usia)[_A-Z]?'\n",
    "    return re.search(pattern, kolom, re.IGNORECASE)\n",
    "\n",
    "def identifikasi_kolom_age(df):\n",
    "    kolom_id = []\n",
    "    for kolom in df.columns:\n",
    "        ratio = df[kolom].nunique() / len(df)\n",
    "        if apakah_kolom_age(kolom):\n",
    "            kolom_id.append(kolom)\n",
    "    return kolom_id\n",
    "\n",
    "print(f\"DF 1 {identifikasi_kolom_age(df1)}\")\n",
    "print(f\"DF 2 {identifikasi_kolom_age(df2)}\")\n",
    "print(f\"DF 3 {identifikasi_kolom_age(df3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81, -88,  94,  26,  35,  25,  54,   0,  80,   1,  10,  21,  36,\n",
       "        44,   6, -71,  61,  85,  97,  63,  31,  90,  78,  34,  93,  59,\n",
       "        32,  47,  52, -79,  77,  66,  92,  15,  33,  16,  43,   9,  20,\n",
       "        83,  24,  56,  45,   3,  79,  65,  51,  57,  95,  72,  84,  71,\n",
       "        42,  13,  22,  87,  60,   7,   4,  50, -11,  41,  74,  30,  46,\n",
       "         8,  12,  49,  17,  82,  29,  96,  38,  11,  99,  23,  88,  70,\n",
       "        73,  40,  19,  28,  53,  98,  86,  76, -95,   5,  91,  69,   2,\n",
       "        37,  64,  27, -39, -25,  14,  55,  48,  62, -28, -62,  89,  68,\n",
       "        67,  18, -99,  75, -57,  39,  58, -48, -13, -69, -58, -68, -55,\n",
       "       -24, -98, -52, -46, -29,  -1, -35, -61, -67, -56, -72, -32, -86,\n",
       "       -47, -36, -40, -22, -12, -45, -33, -91, -23, -31, -74, -97, -10,\n",
       "       -37, -83,  -2, -85, -51,  -4, -38, -17,  -8, -92, -96, -77,  -9,\n",
       "       -21, -20, -63,  -3, -59, -87, -19, -44, -75, -16, -82, -14, -90,\n",
       "       -26, -50, -27, -78, -65, -15, -53, -49,  -6, -54, -84, -93, -34,\n",
       "       -64, -76, -80, -30, -18, -60, -43, -81, -89, -94,  -5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_age(age):\n",
    "    age = int(age)\n",
    "    if age < 0:\n",
    "        return abs(age)\n",
    "    if age == 0:\n",
    "        return age\n",
    "\n",
    "# kolom_age = identifikasi_kolom_age(df3)\n",
    "# for kolom in kolom_age:\n",
    "#     df3[kolom] = df3[kolom].apply(format_age)\n",
    "# df3['Age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Kategorik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Orthopedics' 'Endo' 'Endocrinology' 'Ortho' 'Cardilogy' 'Cardiology'\n",
      " 'Cardio']\n"
     ]
    }
   ],
   "source": [
    "country_mapping = {\n",
    "    'america': 'USA',\n",
    "    'bharat': 'IND',\n",
    "    'uk': 'United Kingdom'\n",
    "}\n",
    "\n",
    "voting_mapping = {\n",
    "    'n': 'No', \n",
    "    'y': 'Yes',\n",
    "    ' ': 'Unknown',\n",
    "    '': 'Unknown',\n",
    "    np.nan: 'Unknown'\n",
    "}\n",
    "\n",
    "marital_mapping = {\n",
    "    's': 'Single',\n",
    "    'belummenikah': 'Single', \n",
    "    'janda': 'Widowed',\n",
    "    'duda': 'Widowed',\n",
    "    'cerai': 'Divorced'\n",
    "}\n",
    "\n",
    "education_mapping = {\n",
    "    'sma': 'High School',\n",
    "    'hs' : 'High School',\n",
    "    'phd' : 'Doctorate',\n",
    "    's1': 'Bachelors',\n",
    "    's2': 'Master',\n",
    "    's3': 'Doctorate'\n",
    "}\n",
    "\n",
    "gender_mapping = {\n",
    "    'm': 'Male',\n",
    "    'f': 'Female',\n",
    "    'u': 'Unknown',\n",
    "}\n",
    "blood_mapping = {\n",
    "    'positive': '+',\n",
    "    'negative': '-',\n",
    "    'positif': '+',\n",
    "    'negatif': '-',\n",
    "}\n",
    "def merge_dictionaries(*dicts):\n",
    "    merged_dict = {}\n",
    "    for dictionary in dicts:\n",
    "        merged_dict.update(dictionary)\n",
    "    return merged_dict\n",
    "\n",
    "def apakah_bloodtype(kolom):\n",
    "    akhiran = kolom.str.endswith('+') | kolom.str.endswith('-')\n",
    "    ada_kata = kolom.str.contains('positive|negative|positif|negatif', case=False, na=False)\n",
    "    return akhiran.any() | ada_kata.any()\n",
    "\n",
    "def format_bloodtype(blood, dictionary):\n",
    "    blood = blood.lower()\n",
    "    bagian = blood.split()\n",
    "    if len(bagian) == 2 and (bagian[0] in ['a', 'b', 'ab', 'o'] and bagian[1] in dictionary):\n",
    "        darah, tipe = bagian\n",
    "        tipe = dictionary.get(tipe.lower(), tipe)\n",
    "        return f\"{darah.upper()}{tipe}\"\n",
    "    elif len(bagian) == 1 and bagian[0][-1] in ['+', '-']:\n",
    "        return bagian[0].upper()\n",
    "    else:\n",
    "        return blood.upper()\n",
    "\n",
    "def replace_values_with_dict(df, dictionary):\n",
    "    for kolom in df.select_dtypes(include=['object']).columns:\n",
    "        df[kolom] = df[kolom].str.strip().str.lower()\n",
    "        df[kolom] = df[kolom].replace(dictionary)\n",
    "        \n",
    "        if apakah_bloodtype(df[kolom]):\n",
    "            df[kolom] = df[kolom].apply(lambda x: format_bloodtype(x, blood_mapping))\n",
    "        else:\n",
    "            df[kolom] = df[kolom].str.title()\n",
    "    return df\n",
    "\n",
    "merged_dict = merge_dictionaries(\n",
    "    country_mapping, \n",
    "    voting_mapping, \n",
    "    marital_mapping,\n",
    "    education_mapping,\n",
    "    gender_mapping,\n",
    "    blood_mapping\n",
    "    )\n",
    "\n",
    "df1 = replace_values_with_dict(df1, merged_dict)\n",
    "df2 = replace_values_with_dict(df2, merged_dict)\n",
    "df3 = replace_values_with_dict(df3, merged_dict)\n",
    "\n",
    "print(df3['Department'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B+', 'O-', 'A+', 'B-', 'A-', 'AB-', 'AB+', 'O+'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['BloodType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = 'A Positive'\n",
    "# z= x.split()\n",
    "# a, b= z\n",
    "# if len(z) == 2 and (bagian[0] in ['a', 'b', 'ab', 'o'] and bagian[1] in dictionary):\n",
    "#         darah, tipe = bagian\n",
    "#         tipe = dictionary.get(tipe.lower(), tipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B+', 'O-', 'A+', 'B-', 'A-', 'AB-', 'AB+', 'O+'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['BloodType'] = df3['BloodType'].apply(lambda x: format_bloodtype(x, blood_mapping))\n",
    "df3['BloodType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male\n",
      "Female\n",
      "Unknown\n"
     ]
    }
   ],
   "source": [
    "for x in gender_mapping.values():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPECIAL WITH JOB,' 'CERTAINLY WAY TEN,' 'STUDY SMALL,' ...\n",
      " 'OLD ENVIRONMENT OFFICIAL LEARN,' 'RECENT COMMUNITY ATTORNEY,'\n",
      " 'SOMEBODY BOTH,']\n",
      "['Female' 'Male' 'Unknown']\n",
      "['B+' 'O-' 'A+' 'B-' 'A-' 'AB-' 'AB+' 'O+']\n"
     ]
    }
   ],
   "source": [
    "print(df3[\"Treatment\"].unique())\n",
    "print(df3[\"Gender\"].unique())\n",
    "print(df3[\"BloodType\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Electronics' 'Clothing' 'Furniture' 'Sports' 'Grocery' 'Electornics'\n",
      " 'Unknown' 'Books']\n",
      "['Appendicitis' 'Fracture' 'Copd' 'Diabetes' 'Hypertension' 'Hypertensyon'\n",
      " 'Dyabetes' 'Appendycytys']\n"
     ]
    }
   ],
   "source": [
    "print(df1['Product_Category'].unique())\n",
    "print(df3['Diagnosis'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Typo\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=4, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "# @lru_cache(maxsize=None)\n",
    "def format_typo(typo):\n",
    "    hasil = str(typo)\n",
    "    saran_typo = sym_spell.lookup(hasil, Verbosity.CLOSEST, max_edit_distance=4)\n",
    "    if saran_typo:\n",
    "        return saran_typo[0].term.title()\n",
    "    return hasil.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Electronics' 'Clothing' 'Furniture' 'Sports' 'Grocery' 'Unknown' 'Books']\n",
      "['Appendicitis' 'Fracture' 'Top' 'Diabetes' 'Hypertension']\n"
     ]
    }
   ],
   "source": [
    "df1['Product_Category'] = df1['Product_Category'].apply(format_typo)\n",
    "df3['Diagnosis'] = df3['Diagnosis'].apply(format_typo)\n",
    "print(df1['Product_Category'].unique())\n",
    "print(df3['Diagnosis'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m sym_spell = SymSpell(max_dictionary_edit_distance=\u001b[32m4\u001b[39m, prefix_length=\u001b[32m10\u001b[39m)\n\u001b[32m      2\u001b[39m dictionary_path = pkg_resources.resource_filename(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msymspellpy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrequency_dictionary_en_82_765.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43msym_spell\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_dictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_index\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount_index\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m kata_benar = {\n\u001b[32m      7\u001b[39m }\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_typo\u001b[39m(typo):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\symspellpy\\symspellpy.py:343\u001b[39m, in \u001b[36mSymSpell.load_dictionary\u001b[39m\u001b[34m(self, corpus, term_index, count_index, separator, encoding)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(corpus, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=encoding) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_dictionary_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\symspellpy\\symspellpy.py:1127\u001b[39m, in \u001b[36mSymSpell._load_dictionary_stream\u001b[39m\u001b[34m(self, corpus_stream, term_index, count_index, separator)\u001b[39m\n\u001b[32m   1125\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1126\u001b[39m     key = parts[term_index]\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_dictionary_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\symspellpy\\symspellpy.py:254\u001b[39m, in \u001b[36mSymSpell.create_dictionary_entry\u001b[39m\u001b[34m(self, key, count)\u001b[39m\n\u001b[32m    252\u001b[39m edits = \u001b[38;5;28mself\u001b[39m._edits_prefix(key)\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m delete \u001b[38;5;129;01min\u001b[39;00m edits:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     \u001b[38;5;28mself\u001b[39m._deletes[delete].append(key)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=4, prefix_length=10)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "kata_benar = {\n",
    "}\n",
    "def format_typo(typo):\n",
    "    if typo in kata_benar:\n",
    "        return kata_benar[typo].title()\n",
    "    saran_typo = sym_spell.lookup(typo, Verbosity.CLOSEST, max_edit_distance=4)\n",
    "    if saran_typo:\n",
    "        jawaban_benar = saran_typo[0].term.title()\n",
    "    else: \n",
    "        jawaban_benar = typo.title()\n",
    "    kata_benar[typo] = jawaban_benar\n",
    "    return jawaban_benar\n",
    "\n",
    "def correct_typo(series):\n",
    "    return series.apply(format_typo)\n",
    "\n",
    "# sym_spell = SymSpell(max_dictionary_edit_distance=4, prefix_length=7)\n",
    "# dictionary_path = pkg_resources.resource_filename(\n",
    "#     \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "# @lru_cache(maxsize=None)\n",
    "# def format_typo(typo):\n",
    "#     hasil = str(typo)\n",
    "#     saran_typo = sym_spell.lookup(hasil, Verbosity.CLOSEST, max_edit_distance=4)\n",
    "#     if saran_typo:\n",
    "#         return saran_typo[0].term.title()\n",
    "#     return hasil.title()\n",
    "\n",
    "# df1['Product_Category'] = df1['Product_Category'].apply(format_typo)\n",
    "df1['Product_Category'] = correct_typo(df1['Product_Category'])\n",
    "df3['Diagnosis'] = correct_typo(df3['Diagnosis'])\n",
    "print(df1['Product_Category'].unique())\n",
    "# df3['Diagnosis'] = df3['Diagnosis'].apply(format_typo)\n",
    "print(df3['Diagnosis'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=10)\n",
    "typo = 'copd'\n",
    "saran_typo = sym_spell.lookup(typo, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "print(saran_typo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell = SpellChecker()\n",
    "# @lru_cache(maxsize=None)\n",
    "# def format_typo(typo):\n",
    "#     hasil = str(spell.correction(typo))\n",
    "#     if hasil.lower() == 'none':\n",
    "#         return f\"{typo} ini woy\"\n",
    "#     return hasil.title()\n",
    "\n",
    "# df1['Product_Category'] = df1['Product_Category'].apply(format_typo)\n",
    "# print(df1['Product_Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def apakah_bloodtype(kolom):\n",
    "    akhiran = kolom.str.endswith('+') | kolom.str.endswith('-')\n",
    "    # ada_kata = kolom.str.contains('positive|negative|positif|negatif', case=False, na=False)\n",
    "    return akhiran.any()\n",
    "\n",
    "hasil = apakah_bloodtype(df3['Medication'])\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_missing_dict_values(series, dictionary, process_func):\n",
    "    \"\"\"\n",
    "    Check if values in a series are not in dictionary and apply a function\n",
    "    \n",
    "    Parameters:\n",
    "    series: pandas Series to check\n",
    "    dictionary: dictionary containing valid values\n",
    "    process_func: function to apply to values not in dictionary\n",
    "    \"\"\"\n",
    "    # Create mask for values not in dictionary\n",
    "    mask = ~series.str.lower().isin([k.lower() for k in dictionary.keys()])\n",
    "    \n",
    "    # Apply function only to values not in dictionary\n",
    "    series.loc[mask] = series.loc[mask].apply(process_func)\n",
    "    \n",
    "    return series\n",
    "\n",
    "def replace_values_with_dict(df, dictionary):\n",
    "    kolom_darah = [col for col in df.columns if apakah_bloodtype(df[col])]\n",
    "    df = df.apply(lambda x: x.str.strip().str.lower() if x.dtype == 'object' else x)\n",
    "    \n",
    "    for kolom in df.select_dtypes(include=['object']).columns:\n",
    "        # First replace values using dictionary\n",
    "        df[kolom] = df[kolom].replace(dictionary)\n",
    "        \n",
    "        # Apply format_typo only to values not in dictionary\n",
    "        df[kolom] = process_missing_dict_values(df[kolom], dictionary, format_typo)\n",
    "        \n",
    "        if kolom in kolom_darah:\n",
    "            df[kolom] = df[kolom].apply(lambda x: format_bloodtype(x, blood_mapping))\n",
    "        else:\n",
    "            df[kolom] = df[kolom].str.title()\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_mapping = {\n",
    "    'america': 'USA',\n",
    "    'bharat': 'IND',\n",
    "    'uk': 'United Kingdom'\n",
    "}\n",
    "voting_mapping = {\n",
    "    'n': 'No', \n",
    "    'y': 'Yes',\n",
    "    ' ': 'Unknown',\n",
    "    '': 'Unknown',\n",
    "    np.nan: 'Unknown'\n",
    "}\n",
    "marital_mapping = {\n",
    "    's': 'Single',\n",
    "    'belummenikah': 'Single', \n",
    "    'janda': 'Widowed',\n",
    "    'duda': 'Widowed',\n",
    "    'cerai': 'Divorced'\n",
    "}\n",
    "education_mapping = {\n",
    "    'sma': 'High School',\n",
    "    'hs' : 'High School',\n",
    "    'phd' : 'Doctorate',\n",
    "    's1': 'Bachelors',\n",
    "    's2': 'Master',\n",
    "    's3': 'Doctorate'\n",
    "}\n",
    "gender_mapping = {\n",
    "    'm': 'Male',\n",
    "    'f': 'Female',\n",
    "    'u': 'Unknown',\n",
    "}\n",
    "blood_mapping = {\n",
    "    'positive': '+',\n",
    "    'negative': '-',\n",
    "    'positif': '+',\n",
    "    'negatif': '-',\n",
    "}\n",
    "\n",
    "def merge_dictionaries(*dicts):\n",
    "    merged_dict = {}\n",
    "    for dictionary in dicts:\n",
    "        merged_dict.update(dictionary)\n",
    "    return merged_dict\n",
    "\n",
    "def apakah_bloodtype(kolom):\n",
    "    akhiran = kolom.str.endswith('+') | kolom.str.endswith('-')\n",
    "    ada_kata = kolom.str.contains('positive|negative|positif|negatif', case=False, na=False)\n",
    "    return akhiran.any() | ada_kata.any()\n",
    "\n",
    "def format_bloodtype(blood, dictionary):\n",
    "    output = blood\n",
    "    bloods = blood.lower()\n",
    "    bagian = bloods.split()\n",
    "    if len(bagian) == 2 and (bagian[0] in ['a', 'b', 'ab', 'o'] and bagian[1] in dictionary):\n",
    "        darah, tipe = bagian\n",
    "        tipe = dictionary.get(tipe.lower(), tipe)\n",
    "        return f\"{darah.upper()}{tipe}\"\n",
    "    elif len(bagian) == 1 and bagian[0][-1] in ['+', '-']:\n",
    "        return bagian[0].upper()\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "# Format Typo\n",
    "spell = SpellChecker()\n",
    "@lru_cache(maxsize=None)\n",
    "def format_typo(typo):\n",
    "    hasil = str(spell.correction(typo))\n",
    "    if hasil.lower() == 'none':\n",
    "        return typo\n",
    "    return hasil.title()\n",
    "\n",
    "def process_missing_dict_values(series, dictionary, process_func):\n",
    "    \"\"\"\n",
    "    Check if values in a series are not in dictionary and apply a function\n",
    "    \n",
    "    Parameters:\n",
    "    series: pandas Series to check\n",
    "    dictionary: dictionary containing valid values\n",
    "    process_func: function to apply to values not in dictionary\n",
    "    \"\"\"\n",
    "    # Create mask for values not in dictionary\n",
    "    mask = ~series.str.lower().isin([k.lower() for k in dictionary.keys()])\n",
    "    \n",
    "    # Apply function only to values not in dictionary\n",
    "    series.loc[mask] = series.loc[mask].apply(process_func)\n",
    "    \n",
    "    return series\n",
    "\n",
    "def replace_values_with_dict(df, dictionary):\n",
    "    kolom_darah = [col for col in df.columns if apakah_bloodtype(df[col])]\n",
    "    df = df.apply(lambda x: x.str.strip().str.lower() if x.dtype == 'object' else x)\n",
    "    \n",
    "    for kolom in df.select_dtypes(include=['object']).columns:\n",
    "        # First replace values using dictionary\n",
    "        df[kolom] = df[kolom].replace(dictionary)\n",
    "        \n",
    "        # Apply format_typo only to values not in dictionary\n",
    "        df[kolom] = process_missing_dict_values(df[kolom], dictionary, format_typo)\n",
    "        \n",
    "        if kolom in kolom_darah:\n",
    "            df[kolom] = df[kolom].apply(lambda x: format_bloodtype(x, blood_mapping))\n",
    "        else:\n",
    "            df[kolom] = df[kolom].str.title()\n",
    "            \n",
    "    return df\n",
    "\n",
    "merged_dict = merge_dictionaries(\n",
    "    country_mapping, \n",
    "    voting_mapping, \n",
    "    marital_mapping,\n",
    "    education_mapping,\n",
    "    gender_mapping,\n",
    "    blood_mapping\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buang Kolom Tidak Perlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2664\n",
      "CustomerID\n",
      "Jumlah 9501\n",
      "Ratio 0.9224271844660195\n",
      "Product_Category\n",
      "Jumlah 7\n",
      "Ratio 0.0006796116504854369\n",
      "Purchase_Date\n",
      "Jumlah 1098\n",
      "Ratio 0.10660194174757281\n",
      "Product_Description\n",
      "Jumlah 10000\n",
      "Ratio 0.970873786407767\n",
      "Country\n",
      "Jumlah 7\n",
      "Ratio 0.0006796116504854369\n",
      "Phone_Number\n",
      "Jumlah 10000\n",
      "Ratio 0.970873786407767\n",
      "Email\n",
      "Jumlah 9831\n",
      "Ratio 0.9544660194174758\n",
      "DF 1 ['CustomerID', 'Purchase_Date', 'Product_Description', 'Phone_Number', 'Email']\n",
      "CitizenID\n",
      "Jumlah 9695\n",
      "Ratio 0.9233333333333333\n",
      "Name\n",
      "Jumlah 9629\n",
      "Ratio 0.917047619047619\n",
      "BirthDate\n",
      "Jumlah 8188\n",
      "Ratio 0.7798095238095238\n",
      "Nationality\n",
      "Jumlah 10\n",
      "Ratio 0.0009523809523809524\n",
      "Address\n",
      "Jumlah 10000\n",
      "Ratio 0.9523809523809523\n",
      "VotingStatus\n",
      "Jumlah 6\n",
      "Ratio 0.0005714285714285715\n",
      "PassportNumber\n",
      "Jumlah 8827\n",
      "Ratio 0.8406666666666667\n",
      "TaxID\n",
      "Jumlah 8956\n",
      "Ratio 0.8529523809523809\n",
      "MaritalStatus\n",
      "Jumlah 6\n",
      "Ratio 0.0005714285714285715\n",
      "CriminalRecord\n",
      "Jumlah 4\n",
      "Ratio 0.00038095238095238096\n",
      "EducationLevel\n",
      "Jumlah 6\n",
      "Ratio 0.0005714285714285715\n",
      "DF 2 ['CitizenID', 'Name', 'BirthDate', 'Address', 'PassportNumber', 'TaxID']\n",
      "PatientID\n",
      "Jumlah 10000\n",
      "Ratio 0.970873786407767\n",
      "AdmissionDate\n",
      "Jumlah 2250\n",
      "Ratio 0.21844660194174756\n",
      "DischargeDate\n",
      "Jumlah 1350\n",
      "Ratio 0.13106796116504854\n",
      "Diagnosis\n",
      "Jumlah 13\n",
      "Ratio 0.001262135922330097\n",
      "Treatment\n",
      "Jumlah 9679\n",
      "Ratio 0.9397087378640777\n",
      "Doctor\n",
      "Jumlah 8489\n",
      "Ratio 0.8241747572815534\n",
      "Department\n",
      "Jumlah 7\n",
      "Ratio 0.0006796116504854369\n",
      "Gender\n",
      "Jumlah 5\n",
      "Ratio 0.0004854368932038835\n",
      "BloodType\n",
      "Jumlah 10\n",
      "Ratio 0.000970873786407767\n",
      "Medication\n",
      "Jumlah 2664\n",
      "Ratio 0.25864077669902913\n",
      "TestResults\n",
      "Jumlah 5993\n",
      "Ratio 0.5818446601941748\n",
      "InsuranceProvider\n",
      "Jumlah 4\n",
      "Ratio 0.0003883495145631068\n",
      "AppointmentDate\n",
      "Jumlah 731\n",
      "Ratio 0.07097087378640776\n",
      "DF 3 ['PatientID', 'AdmissionDate', 'DischargeDate', 'Treatment', 'Doctor', 'Medication', 'TestResults', 'AppointmentDate']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0           1\n",
       "1           2\n",
       "2           3\n",
       "3           4\n",
       "4           5\n",
       "         ... \n",
       "10295    7431\n",
       "10296     940\n",
       "10297    9519\n",
       "10298    1804\n",
       "10299    8721\n",
       "Name: ID, Length: 10300, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"Raw Data/custdirty_data.csv\")\n",
    "df2 = pd.read_csv(\"Raw Data/government_citizenship_dirty.csv\")\n",
    "df3 = pd.read_csv(\"Raw Data/hospital_data_dirty.csv\")\n",
    "def format_kolom_hapus(df, threshold = 0.6):\n",
    "    kolom_id = []\n",
    "    for kolom in df.select_dtypes(include=['object']).columns:\n",
    "        ratio = df[kolom].nunique() / len(df)\n",
    "        jumlah = df[kolom].nunique()\n",
    "        print(kolom)\n",
    "        print(f'Jumlah {jumlah}')\n",
    "        print(f'Ratio {ratio}')\n",
    "        if ratio >= threshold or jumlah > 100:\n",
    "            kolom_id.append(kolom)\n",
    "    return kolom_id\n",
    "\n",
    "print(df3['Medication'].nunique())\n",
    "print(f\"DF 1 {format_kolom_hapus(df1)}\")\n",
    "print(f\"DF 2 {format_kolom_hapus(df2)}\")\n",
    "print(f\"DF 3 {format_kolom_hapus(df3)}\")\n",
    "df1['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2024-10-03\n",
       "1         2023-06-27\n",
       "2         2024-05-18\n",
       "3        Mar 11 1978\n",
       "4         2024-11-23\n",
       "            ...     \n",
       "10295     2024-03-16\n",
       "10296     2023-03-29\n",
       "10297    Oct 11 1998\n",
       "10298     2024-02-01\n",
       "10299    Mar 31 2017\n",
       "Name: AdmissionDate, Length: 10300, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['AdmissionDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product_Category\n",
      "Purchase_Date\n",
      "Product_Description\n",
      "Country\n",
      "Phone_Number\n",
      "Email\n",
      "Name\n",
      "Nationality\n",
      "Address\n",
      "VotingStatus\n",
      "PassportNumber\n",
      "MaritalStatus\n",
      "CriminalRecord\n",
      "EducationLevel\n",
      "AdmissionDate\n",
      "DischargeDate\n",
      "Diagnosis\n",
      "Treatment\n",
      "Doctor\n",
      "Department\n",
      "Gender\n",
      "BloodType\n",
      "Medication\n",
      "TestResults\n",
      "InsuranceProvider\n",
      "AppointmentDate\n",
      "object\n",
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "for kolom in df1.select_dtypes(include=['object']).columns:\n",
    "    print(kolom)\n",
    "for kolom in df2.select_dtypes(include=['object']).columns:\n",
    "    print(kolom)\n",
    "for kolom in df3.select_dtypes(include=['object']).columns:\n",
    "    print(kolom)\n",
    "print(df3['AdmissionDate'].dtype)\n",
    "print(df3['DischargeDate'].dtype)\n",
    "print(df3['AppointmentDate'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setelah Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Category</th>\n",
       "      <th>Purchase_Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>2024-03-05</td>\n",
       "      <td>65.58</td>\n",
       "      <td>11.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>335.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>2024-05-28</td>\n",
       "      <td>106.92</td>\n",
       "      <td>19.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clothing</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>389.70</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Furniture</td>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>140.78</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Furniture</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>435.00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Grocery</td>\n",
       "      <td>2024-07-16</td>\n",
       "      <td>56.34</td>\n",
       "      <td>15.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>2024-11-23</td>\n",
       "      <td>444.24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>41.64</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>2024-09-24</td>\n",
       "      <td>285.31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Product_Category Purchase_Date   Price  Quantity         Country\n",
       "0         Electronics    2024-03-05   65.58      11.0   United States\n",
       "1         Electronics    2024-05-29  335.85       1.0          Canada\n",
       "2         Electronics    2024-05-28  106.92      19.0  United Kingdom\n",
       "3            Clothing    2024-06-30  389.70       6.0          Canada\n",
       "4           Furniture    2024-10-14  140.78      14.0          Canada\n",
       "...               ...           ...     ...       ...             ...\n",
       "9995        Furniture    2025-01-23  435.00      11.0   United States\n",
       "9996          Grocery    2024-07-16   56.34      15.0   United States\n",
       "9997      Electronics    2024-11-23  444.24       1.0   United States\n",
       "9998          Unknown    2024-06-15   41.64       5.0          Canada\n",
       "9999      Electronics    2024-09-24  285.31       1.0          Canada\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_baru = pd.read_csv(\"clean-custdirty_data.csv\")\n",
    "df2_baru = pd.read_csv(\"clean-government_citizenship_dirty.csv\")\n",
    "df3_baru = pd.read_csv(\"clean-hospital_data_dirty.csv\")\n",
    "\n",
    "df1_baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>VotingStatus</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>CriminalRecord</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>Income</th>\n",
       "      <th>ImmigrationYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1968-06-16</td>\n",
       "      <td>United States</td>\n",
       "      <td>No</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>High School</td>\n",
       "      <td>42533.54</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987-02-11</td>\n",
       "      <td>France</td>\n",
       "      <td>No</td>\n",
       "      <td>Single</td>\n",
       "      <td>No</td>\n",
       "      <td>Masters</td>\n",
       "      <td>28175.78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1937-12-15</td>\n",
       "      <td>India</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>High School</td>\n",
       "      <td>143299.35</td>\n",
       "      <td>1986.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1991-03-24</td>\n",
       "      <td>United States</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>105629.33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-10-15</td>\n",
       "      <td>France</td>\n",
       "      <td>No</td>\n",
       "      <td>Single</td>\n",
       "      <td>Pending</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>117985.07</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9494</th>\n",
       "      <td>1997-09-15</td>\n",
       "      <td>France</td>\n",
       "      <td>No</td>\n",
       "      <td>Single</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>High School</td>\n",
       "      <td>24575.24</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9495</th>\n",
       "      <td>1950-04-30</td>\n",
       "      <td>India</td>\n",
       "      <td>Registered</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>No</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>67548.03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9496</th>\n",
       "      <td>1997-02-25</td>\n",
       "      <td>India</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Married</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>72134.76</td>\n",
       "      <td>1972.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9497</th>\n",
       "      <td>1971-12-18</td>\n",
       "      <td>France</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Single</td>\n",
       "      <td>No</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>90587.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9498</th>\n",
       "      <td>1959-08-24</td>\n",
       "      <td>United States</td>\n",
       "      <td>No</td>\n",
       "      <td>Single</td>\n",
       "      <td>Pending</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>47292.07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9499 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BirthDate    Nationality VotingStatus MaritalStatus CriminalRecord  \\\n",
       "0     1968-06-16  United States           No       Married             No   \n",
       "1     1987-02-11         France           No        Single             No   \n",
       "2     1937-12-15          India          Yes       Widowed        Unknown   \n",
       "3     1991-03-24  United States          Yes       Married             No   \n",
       "4     2000-10-15         France           No        Single        Pending   \n",
       "...          ...            ...          ...           ...            ...   \n",
       "9494  1997-09-15         France           No        Single        Unknown   \n",
       "9495  1950-04-30          India   Registered       Unknown             No   \n",
       "9496  1997-02-25          India      Unknown       Married            Yes   \n",
       "9497  1971-12-18         France          Yes        Single             No   \n",
       "9498  1959-08-24  United States           No        Single        Pending   \n",
       "\n",
       "     EducationLevel     Income  ImmigrationYear  \n",
       "0       High School   42533.54              NaN  \n",
       "1           Masters   28175.78              NaN  \n",
       "2       High School  143299.35           1986.0  \n",
       "3         Doctorate  105629.33              NaN  \n",
       "4         Doctorate  117985.07           2005.0  \n",
       "...             ...        ...              ...  \n",
       "9494    High School   24575.24           1999.0  \n",
       "9495      Bachelors   67548.03              NaN  \n",
       "9496    High School   72134.76           1972.0  \n",
       "9497      Doctorate   90587.75              NaN  \n",
       "9498      Bachelors   47292.07              NaN  \n",
       "\n",
       "[9499 rows x 8 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdmissionDate</th>\n",
       "      <th>DischargeDate</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>Department</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BloodType</th>\n",
       "      <th>BillingAmount</th>\n",
       "      <th>InsuranceProvider</th>\n",
       "      <th>AppointmentDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-03</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>Appendicitis</td>\n",
       "      <td>Orthopedics</td>\n",
       "      <td>81</td>\n",
       "      <td>Female</td>\n",
       "      <td>B+</td>\n",
       "      <td>9495.73</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2024-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-27</td>\n",
       "      <td>2023-12-05</td>\n",
       "      <td>Fracture</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>88</td>\n",
       "      <td>Male</td>\n",
       "      <td>O-</td>\n",
       "      <td>8681.08</td>\n",
       "      <td>Coinsurance</td>\n",
       "      <td>2023-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>2025-04-29</td>\n",
       "      <td>Appendicitis</td>\n",
       "      <td>Orthopedics</td>\n",
       "      <td>94</td>\n",
       "      <td>Male</td>\n",
       "      <td>O-</td>\n",
       "      <td>2829.42</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2024-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1978-03-11</td>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>26</td>\n",
       "      <td>Female</td>\n",
       "      <td>A+</td>\n",
       "      <td>1121.27</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2024-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-23</td>\n",
       "      <td>2024-12-02</td>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Orthopedics</td>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>B-</td>\n",
       "      <td>5555.75</td>\n",
       "      <td>Coinsurance</td>\n",
       "      <td>2023-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>2024-11-06</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Fracture</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>60</td>\n",
       "      <td>Female</td>\n",
       "      <td>A-</td>\n",
       "      <td>5537.98</td>\n",
       "      <td>Coinsurance</td>\n",
       "      <td>2024-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7026</th>\n",
       "      <td>2023-07-19</td>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>A+</td>\n",
       "      <td>3703.67</td>\n",
       "      <td>Healthful</td>\n",
       "      <td>2024-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027</th>\n",
       "      <td>2023-04-18</td>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>90</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>O+</td>\n",
       "      <td>5659.97</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2024-07-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7028</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>2024-11-14</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>B-</td>\n",
       "      <td>3511.16</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2024-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7029</th>\n",
       "      <td>2024-04-12</td>\n",
       "      <td>2024-12-21</td>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>59</td>\n",
       "      <td>Female</td>\n",
       "      <td>AB+</td>\n",
       "      <td>3445.49</td>\n",
       "      <td>Coinsurance</td>\n",
       "      <td>2024-10-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7030 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AdmissionDate DischargeDate     Diagnosis     Department  Age   Gender  \\\n",
       "0       2024-10-03    2025-05-11  Appendicitis    Orthopedics   81   Female   \n",
       "1       2023-06-27    2023-12-05      Fracture  Endocrinology   88     Male   \n",
       "2       2024-05-18    2025-04-29  Appendicitis    Orthopedics   94     Male   \n",
       "3       1978-03-11    2019-10-26          Cold  Endocrinology   26   Female   \n",
       "4       2024-11-23    2024-12-02      Diabetes    Orthopedics   35   Female   \n",
       "...            ...           ...           ...            ...  ...      ...   \n",
       "7025    2024-11-06    2025-03-01      Fracture  Endocrinology   60   Female   \n",
       "7026    2023-07-19    2024-09-16          Cold  Endocrinology    3     Male   \n",
       "7027    2023-04-18    2023-06-08      Diabetes     Cardiology   90  Unknown   \n",
       "7028    2024-01-03    2024-11-14          Cold  Endocrinology   35   Female   \n",
       "7029    2024-04-12    2024-12-21      Diabetes     Cardiology   59   Female   \n",
       "\n",
       "     BloodType  BillingAmount InsuranceProvider AppointmentDate  \n",
       "0           B+        9495.73           Unknown      2024-02-17  \n",
       "1           O-        8681.08       Coinsurance      2023-08-08  \n",
       "2           O-        2829.42           Unknown      2024-05-21  \n",
       "3           A+        1121.27           Unknown      2024-11-03  \n",
       "4           B-        5555.75       Coinsurance      2023-11-14  \n",
       "...        ...            ...               ...             ...  \n",
       "7025        A-        5537.98       Coinsurance      2024-12-03  \n",
       "7026        A+        3703.67         Healthful      2024-07-03  \n",
       "7027        O+        5659.97           Unknown      2024-07-29  \n",
       "7028        B-        3511.16           Unknown      2024-12-14  \n",
       "7029       AB+        3445.49       Coinsurance      2024-10-11  \n",
       "\n",
       "[7030 rows x 10 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Orthopaedics' 'Endocrinology' 'Cardiology']\n",
      "['Appendicitis' 'Fracture' 'Cold' 'Diabetes' 'Hypertension']\n"
     ]
    }
   ],
   "source": [
    "print(df3_baru['Department'].unique())\n",
    "print(df3_baru['Diagnosis'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    135\u001b[39m merged_dict = merge_dictionaries(\n\u001b[32m    136\u001b[39m     country_mapping, \n\u001b[32m    137\u001b[39m     voting_mapping, \n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m     medics_mapping\n\u001b[32m    143\u001b[39m     )\n\u001b[32m    145\u001b[39m df3_baru = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mclean-hospital_data_dirty.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m df3_baru = \u001b[43mreplace_values_with_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf3_baru\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28mprint\u001b[39m(df3_baru[\u001b[33m'\u001b[39m\u001b[33mDepartment\u001b[39m\u001b[33m'\u001b[39m].unique())\n\u001b[32m    148\u001b[39m \u001b[38;5;28mprint\u001b[39m(df3_baru[\u001b[33m'\u001b[39m\u001b[33mDiagnosis\u001b[39m\u001b[33m'\u001b[39m].unique())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mreplace_values_with_dict\u001b[39m\u001b[34m(df, dictionary)\u001b[39m\n\u001b[32m    122\u001b[39m df[kolom] = df[kolom].replace(dictionary)\n\u001b[32m    123\u001b[39m unique_values = df[kolom].unique()\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m corrections = {val: \u001b[43mcombine_format_typo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dictionary \u001b[38;5;28;01melse\u001b[39;00m val \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m unique_values}\n\u001b[32m    125\u001b[39m df[kolom] = df[kolom].map(corrections)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# print(kolom)\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# df[kolom] = df[kolom].apply(lambda x: combine_format_typo(x) if x not in dictionary else x)\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# df[kolom] = df[kolom].apply(lambda x: combine_format_typo(x))\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mcombine_format_typo\u001b[39m\u001b[34m(typo)\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cache_kata[typo]\n\u001b[32m    104\u001b[39m typo_lower = \u001b[38;5;28mstr\u001b[39m(typo).lower()\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m result = \u001b[43mspell\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypo_lower\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result \u001b[38;5;129;01mor\u001b[39;00m result == typo_lower:\n\u001b[32m    107\u001b[39m     suggestions = sym_spell.lookup(typo_lower, Verbosity.CLOSEST, max_edit_distance=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\spellchecker\\spellchecker.py:158\u001b[39m, in \u001b[36mSpellChecker.correction\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The most probable correct spelling for the word\u001b[39;00m\n\u001b[32m    152\u001b[39m \n\u001b[32m    153\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m    word (str): The word to correct\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[33;03m    str: The most likely candidate or None if no correction is present\"\"\"\u001b[39;00m\n\u001b[32m    157\u001b[39m word = ensure_unicode(word)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m candidates = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\spellchecker\\spellchecker.py:185\u001b[39m, in \u001b[36mSpellChecker.candidates\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._distance == \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     tmp = \u001b[38;5;28mself\u001b[39m.known(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[32m    187\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\spellchecker\\spellchecker.py:252\u001b[39m, in \u001b[36mSpellChecker.__edit_distance_alt\u001b[39m\u001b[34m(self, words)\u001b[39m\n\u001b[32m    250\u001b[39m tmp_words = [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[32m    251\u001b[39m tmp = [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w.lower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_if_should_check(w)]\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\spellchecker\\spellchecker.py:198\u001b[39m, in \u001b[36mSpellChecker.known\u001b[39m\u001b[34m(self, words)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[32m    192\u001b[39m \n\u001b[32m    193\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[33;03m    words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m    set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m tmp_words = [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m tmp = [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._case_sensitive \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._word_frequency.dictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_if_should_check(w)}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "country_mapping = {\n",
    "    'america': 'USA',\n",
    "    'bharat': 'IND',\n",
    "    'uk': 'United Kingdom'\n",
    "}\n",
    "voting_mapping = {\n",
    "    'n': 'No', \n",
    "    'y': 'Yes',\n",
    "    ' ': 'Unknown',\n",
    "    '': 'Unknown',\n",
    "}\n",
    "marital_mapping = {\n",
    "    's': 'Single',\n",
    "    'belummenikah': 'Single', \n",
    "    'janda': 'Widowed',\n",
    "    'duda': 'Widowed',\n",
    "    'cerai': 'Divorced'\n",
    "}\n",
    "education_mapping = {\n",
    "    'sma': 'High School',\n",
    "    'hs' : 'High School',\n",
    "    'phd' : 'Doctorate',\n",
    "    's1': 'Bachelors',\n",
    "    's2': 'Master',\n",
    "    's3': 'Doctorate'\n",
    "}\n",
    "gender_mapping = {\n",
    "    'm': 'Male',\n",
    "    'f': 'Female',\n",
    "    'u': 'Unknown',\n",
    "}\n",
    "blood_mapping = {\n",
    "    'positive': '+',\n",
    "    'negative': '-',\n",
    "    'positif': '+',\n",
    "    'negatif': '-',\n",
    "}\n",
    "medics_mapping = {\n",
    "    \"ortho\": 'Orthopedics',\n",
    "    \"neuro\": 'Neurology',\n",
    "    \"gastro\": 'Gastroenterology',\n",
    "    \"cardio\": 'Cardiology',\n",
    "    \"onco\": 'Oncology',\n",
    "    \"endo\": 'Endocrinology',\n",
    "    \"uro\": 'Urology',\n",
    "    \"nephro\": 'Nephrology',\n",
    "    \"pulmo\": 'Pulmonology',\n",
    "    \"derma\": 'Dermatology',\n",
    "    \"pedi\": 'Pediatrics',\n",
    "    \"psych\": 'Psychiatry',\n",
    "    \"radi\": 'Radiology',\n",
    "    \"patho\": 'Pathology',\n",
    "    \"anest\": 'Anesthesiology',\n",
    "    \"gen\": 'General',\n",
    "}\n",
    "def merge_dictionaries(*dicts):\n",
    "    merged_dict = {}\n",
    "    for dictionary in dicts:\n",
    "        merged_dict.update(dictionary)\n",
    "    return merged_dict\n",
    "\n",
    "def apakah_bloodtype(kolom):\n",
    "    kolom = kolom.astype(str)\n",
    "    akhiran = kolom.str.endswith('+') | kolom.str.endswith('-')\n",
    "    ada_kata = kolom.str.contains('positive|negative|positif|negatif', case=False, na=False)\n",
    "    return akhiran.any() | ada_kata.any()\n",
    "\n",
    "def format_bloodtype(blood, dictionary):\n",
    "    output = blood\n",
    "    bloods = blood.lower()\n",
    "    bagian = bloods.split()\n",
    "    if len(bagian) == 2 and (bagian[0] in ['a', 'b', 'ab', 'o'] and bagian[1] in dictionary):\n",
    "        darah, tipe = bagian\n",
    "        tipe = dictionary.get(tipe.lower(), tipe)\n",
    "        return f\"{darah.upper()}{tipe}\"\n",
    "    elif len(bagian) == 1 and bagian[0][-1] in ['+', '-']:\n",
    "        return bagian[0].upper()\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "# Format Typo\n",
    "spell = SpellChecker()\n",
    "@lru_cache(maxsize=None)\n",
    "def format_typo1(typo):\n",
    "    hasil = spell.correction(typo)\n",
    "    if hasil is None:\n",
    "        return typo.title()\n",
    "    return hasil.title()\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=4, prefix_length=12)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "def format_typo2(typo):\n",
    "    saran_typo = sym_spell.lookup(typo, Verbosity.CLOSEST, max_edit_distance=4)\n",
    "    if saran_typo:\n",
    "        return saran_typo[0].term.title()\n",
    "    # return hasil.title()\n",
    "\n",
    "cache_kata = {}\n",
    "def combine_format_typo(typo):\n",
    "    if typo in cache_kata:\n",
    "        return cache_kata[typo]\n",
    "    typo_lower = str(typo).lower()\n",
    "    result = spell.correction(typo_lower)\n",
    "    if not result or result == typo_lower:\n",
    "        suggestions = sym_spell.lookup(typo_lower, Verbosity.CLOSEST, max_edit_distance=4)\n",
    "        if suggestions:\n",
    "            result = suggestions[0].term\n",
    "        else:\n",
    "            result = typo_lower\n",
    "    result = result.title()\n",
    "    cache_kata[typo] = result\n",
    "    return result\n",
    "\n",
    "def replace_values_with_dict(df, dictionary):\n",
    "    kolom_darah = [col for col in df.columns if apakah_bloodtype(df[col])]\n",
    "    \n",
    "    for kolom in df.select_dtypes(include=['object']).columns:\n",
    "        df[kolom] = df[kolom].fillna('Unknown')\n",
    "        df[kolom] = df[kolom].apply(lambda x: str(x).lower())\n",
    "        df[kolom] = df[kolom].replace(dictionary)\n",
    "        # unique_values = df[kolom].unique()\n",
    "        # corrections = {val: combine_format_typo(val) if val not in dictionary else val for val in unique_values}\n",
    "        # df[kolom] = df[kolom].map(corrections)\n",
    "        # print(kolom)\n",
    "        df[kolom] = df[kolom].apply(lambda x: combine_format_typo(x) if x not in dictionary else x)\n",
    "        # df[kolom] = df[kolom].apply(lambda x: combine_format_typo(x))\n",
    "        if kolom in kolom_darah:\n",
    "            df[kolom] = df[kolom].apply(lambda x: format_bloodtype(x, blood_mapping))\n",
    "        else:\n",
    "            df[kolom] = df[kolom].str.title()\n",
    "    return df\n",
    "\n",
    "merged_dict = merge_dictionaries(\n",
    "    country_mapping, \n",
    "    voting_mapping, \n",
    "    marital_mapping,\n",
    "    education_mapping,\n",
    "    gender_mapping,\n",
    "    blood_mapping,\n",
    "    medics_mapping\n",
    "    )\n",
    "\n",
    "df3_baru = pd.read_csv(\"clean-hospital_data_dirty.csv\")\n",
    "df3_baru = replace_values_with_dict(df3_baru, merged_dict)\n",
    "print(df3_baru['Department'].unique())\n",
    "print(df3_baru['Diagnosis'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=4, prefix_length=12)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "# Cache for storing already checked words\n",
    "# word_cache = {}\n",
    "\n",
    "# def combine_format_typo(typo):\n",
    "#     # Check cache first\n",
    "#     if typo in word_cache:\n",
    "#         return word_cache[typo]\n",
    "    \n",
    "#     # Convert to lowercase for consistency\n",
    "#     typo_lower = str(typo).lower()\n",
    "    \n",
    "#     # Try pyspellchecker first (better for common words)\n",
    "#     result = spell.correction(typo_lower)\n",
    "#     if not result or result == typo_lower:\n",
    "#         # Fallback to symspell for more aggressive correction\n",
    "#         suggestions = sym_spell.lookup(typo_lower, Verbosity.CLOSEST, max_edit_distance=4)\n",
    "#         if suggestions:\n",
    "#             result = suggestions[0].term\n",
    "#         else:\n",
    "#             result = typo_lower\n",
    "    \n",
    "#     # Title case the result\n",
    "#     result = result.title()\n",
    "    \n",
    "#     # Cache the result\n",
    "#     word_cache[typo] = result\n",
    "#     return result\n",
    "\n",
    "cache_kata = {}\n",
    "def combine_format_typo(typo):\n",
    "    if typo in cache_kata:\n",
    "        return cache_kata[typo]\n",
    "    typo_lower = str(typo).lower()\n",
    "    result = spell.correction(typo_lower)\n",
    "    if not result or result == typo_lower:\n",
    "        suggestions = sym_spell.lookup(typo_lower, Verbosity.CLOSEST, max_edit_distance=4)\n",
    "        if suggestions:\n",
    "            result = suggestions[0].term\n",
    "        else:\n",
    "            result = typo_lower\n",
    "    result = result.title()\n",
    "    cache_kata[typo] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Appendicitis', 'Fracture', 'Cold', 'Diabetes', 'Hypertension'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_baru['Diagnosis'] = df3_baru['Diagnosis'].apply(combine_format_typo)\n",
    "df3_baru['Diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Orthopedics', 'Endocrinology', 'Orthopedic', 'Cardiology',\n",
       "       'Cardilogy'], dtype=object)"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_baru['Department'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[613]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hasil.title()\n\u001b[32m      7\u001b[39m df3_baru[\u001b[33m'\u001b[39m\u001b[33mDepartment\u001b[39m\u001b[33m'\u001b[39m] = df3_baru[\u001b[33m'\u001b[39m\u001b[33mDepartment\u001b[39m\u001b[33m'\u001b[39m].apply(format_typo1)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df3_baru[\u001b[33m'\u001b[39m\u001b[33mDiagnosis\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf3_baru\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDiagnosis\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_typo1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(df3_baru[\u001b[33m'\u001b[39m\u001b[33mDepartment\u001b[39m\u001b[33m'\u001b[39m].unique())\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(df3_baru[\u001b[33m'\u001b[39m\u001b[33mDiagnosis\u001b[39m\u001b[33m'\u001b[39m].unique())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[613]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mformat_typo1\u001b[39m\u001b[34m(typo)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_typo1\u001b[39m(typo):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     hasil = \u001b[43mspell\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hasil \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m typo.title()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\spellchecker\\spellchecker.py:158\u001b[39m, in \u001b[36mSpellChecker.correction\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The most probable correct spelling for the word\u001b[39;00m\n\u001b[32m    152\u001b[39m \n\u001b[32m    153\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m    word (str): The word to correct\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[33;03m    str: The most likely candidate or None if no correction is present\"\"\"\u001b[39;00m\n\u001b[32m    157\u001b[39m word = ensure_unicode(word)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m candidates = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\spellchecker\\spellchecker.py:185\u001b[39m, in \u001b[36mSpellChecker.candidates\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._distance == \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     tmp = \u001b[38;5;28mself\u001b[39m.known(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[32m    187\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\spellchecker\\spellchecker.py:252\u001b[39m, in \u001b[36mSpellChecker.__edit_distance_alt\u001b[39m\u001b[34m(self, words)\u001b[39m\n\u001b[32m    250\u001b[39m tmp_words = [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[32m    251\u001b[39m tmp = [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w.lower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_if_should_check(w)]\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\spellchecker\\spellchecker.py:199\u001b[39m, in \u001b[36mSpellChecker.known\u001b[39m\u001b[34m(self, words)\u001b[39m\n\u001b[32m    197\u001b[39m tmp_words = [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[32m    198\u001b[39m tmp = [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w.lower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_word_frequency\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdictionary\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_if_should_check(w)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\project\\Lib\\site-packages\\spellchecker\\spellchecker.py:328\u001b[39m, in \u001b[36mWordFrequency.dictionary\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    325\u001b[39m     key = ensure_unicode(key)\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dictionary.pop(key \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._case_sensitive \u001b[38;5;28;01melse\u001b[39;00m key.lower(), default)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdictionary\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    330\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Counter: A counting dictionary of all words in the corpus and the number\u001b[39;00m\n\u001b[32m    331\u001b[39m \u001b[33;03m    of times each has been seen\u001b[39;00m\n\u001b[32m    332\u001b[39m \n\u001b[32m    333\u001b[39m \u001b[33;03m    Note:\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m        Not settable\"\"\"\u001b[39;00m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dictionary\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df3_baru = pd.read_csv(\"clean-hospital_data_dirty.csv\")\n",
    "def format_typo1(typo):\n",
    "    hasil = spell.correction(typo)\n",
    "    if hasil is None:\n",
    "        return typo.title()\n",
    "    return hasil.title()\n",
    "df3_baru['Department'] = df3_baru['Department'].apply(format_typo1)\n",
    "# df3_baru['Diagnosis'] = df3_baru['Diagnosis'].apply(format_typo1)\n",
    "print(df3_baru['Department'].unique())\n",
    "# print(df3_baru['Diagnosis'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product_Category\n",
      "Purchase_Date\n",
      "Product_Description\n",
      "Country\n",
      "Phone_Number\n",
      "Email\n",
      "Name\n",
      "BirthDate\n",
      "Nationality\n",
      "Address\n",
      "VotingStatus\n",
      "PassportNumber\n",
      "MaritalStatus\n",
      "CriminalRecord\n",
      "EducationLevel\n",
      "AdmissionDate\n",
      "DischargeDate\n",
      "Diagnosis\n",
      "Department\n",
      "Gender\n",
      "BloodType\n",
      "InsuranceProvider\n",
      "AppointmentDate\n",
      "object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      2024-02-17\n",
       "1      2023-08-08\n",
       "2      2024-05-21\n",
       "3      2024-11-03\n",
       "4      2023-11-14\n",
       "          ...    \n",
       "7025   2024-12-03\n",
       "7026   2024-07-03\n",
       "7027   2024-07-29\n",
       "7028   2024-12-14\n",
       "7029   2024-10-11\n",
       "Name: AppointmentDate, Length: 7030, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for kolom in df1_baru.select_dtypes(include=['object']).columns:\n",
    "    print(kolom)\n",
    "for kolom in df2_baru.select_dtypes(include=['object']).columns:\n",
    "    print(kolom)\n",
    "for kolom in df3_baru.select_dtypes(include=['object']).columns:\n",
    "    print(kolom)\n",
    "# print(df3_baru['AdmissionDate'].dtype)\n",
    "# print(df3_baru['DischargeDate'].dtype)\n",
    "print(df3_baru['AppointmentDate'].dtype)\n",
    "\n",
    "# df3_baru['AppointmentDate'].isna().sum() \n",
    "# df3_baru['AppointmentDate'] \n",
    "\n",
    "def safe_parse(date_str):\n",
    "    date_str = str(date_str).strip()\n",
    "    try:\n",
    "        parsed_date = parser.parse(date_str)\n",
    "        return pd.to_datetime(parsed_date)\n",
    "    except (ValueError, TypeError): \n",
    "        return pd.NaT \n",
    "\n",
    "df3_baru['AppointmentDate'] = df3_baru['AppointmentDate'].apply(safe_parse)\n",
    "df3_baru['AppointmentDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20327169274537696\n",
      "AdmissionDate\n",
      "0.19203413940256045\n",
      "DischargeDate\n",
      "0.0011379800853485065\n",
      "Diagnosis\n",
      "0.000995732574679943\n",
      "Department\n",
      "0.01422475106685633\n",
      "Age\n",
      "0.0004267425320056899\n",
      "Gender\n",
      "0.0011379800853485065\n",
      "BloodType\n",
      "0.9706970128022759\n",
      "BillingAmount\n",
      "0.0007112375533428165\n",
      "InsuranceProvider\n",
      "0.10398293029871977\n",
      "AppointmentDate\n"
     ]
    }
   ],
   "source": [
    "def tes(df):\n",
    "    for kolom in df.columns:\n",
    "        ratio = df[kolom].nunique() / len(df)\n",
    "        print(ratio)\n",
    "        print(kolom)\n",
    "tes(df3_baru)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
